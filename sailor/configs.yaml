defaults:  
  # Directory Configs
  scratch_dir: "scratch_dir/"
  datadir: null # Set automatically using the suite

  # General Configs
  seed: 0
  deterministic_run: False
  compile: True
  device: 'cuda:0'
  debug: False
  precision: 32

  # Wandb configs
  use_wandb: True
  wandb_entity: 'mredmondwang' # Top level name
  wandb_project: null
  wandb_exp_name: null

  # Run Params
  viz_expert_buffer: False
  update_dp: False
  train_dp_mppi: False
  state_only: False
  generate_highres_eval: False

  # Data params
  num_exp_trajs: -1 # To be specified when starting a new run
  num_exp_val_trajs: 1
  normalize_state_actions: False
  image_size: 64
  obs_horizon: 2 # Number of observations to stack
  action_horizon: 1 # Number of actions to take
  pred_horizon: 8 # Number of actions to stack
  num_buffer_transitions: 100000
  state_dim: -1 # To be set when loading expert data
  action_dim: -1 # To be set when loading expert data

  # Evaluation
  eval_num_runs: 50
  visualize_eval: True # Store videos of all eval runs
  log_every: 50

  # Environment
  task: null
  num_envs: 10
  time_limit: -1 # overwritten for each env, defined below
  high_res_render: False # If true, videos will be stored in high resolution
  highres_img_size: 1024
  
  # Model
  reward_EMA: True
  dyn_hidden: 1024
  dyn_deter: 1024
  dyn_stoch: 32
  dyn_discrete: 64 # set as 0 for continuous latent
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'cont']
  units: 1024
  act: 'SiLU'
  norm: True'
  encoder:
    {mlp_keys: '.*state.*', cnn_keys: '.*image.*', act: 'SiLU', norm: True, cnn_depth: 64, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '.*state.*', cnn_keys: '.*image.*', act: 'SiLU', norm: True, cnn_depth: 64, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  residual_actor:
    {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.01, max_std: 0.1, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0, absmax: null}
  critic:
    {num_models: 5, num_subsample: 2, layers: 2, dist: 'normal_std_fixed', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'normal_std_fixed', outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 16
  batch_length: 32
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  opt: 'adam'
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 7

  # DP Config
  dp:
    train_steps: 1000000
    batch_size: 256
    lr: 0.0001
    max_iterations: 500000
    log_freq: 100
    eval_freq: 5000
    schedule_freq: 1
    devices: 1
    device: 'cuda:0'
    seed: 292285
    action_space: 'deltas'
    ac_chunk: 8
    img_chunk: 2
    num_cams: 2
    train_transform: 'gpu_medium'
    debug: False
    print_config: False
    pretrained_ckpt: "DP_Pretrain_base_policy_latest.pt"
    shared_mlp: []
    use_if_ckpt_present: False
  
  # MPPI config
  mppi:
    # planning
    iterations: 6
    num_elites: 32
    temperature: 0.5

    # sampling
    num_samples: 256
    init_std: 0.05
    abs_residual: 0.2
    max_std: 0.1
    min_std: 0.01
    uncertainty_cost: 1.0
    action_l2_cost: 0

    # learning
    horizon: 8
    discount: 0.99

    pretrained_ckpt: ""
  
  # Train DP with MPPI
  train_dp_mppi_params:
    # Warmstart
    force_warmup: True # TODO: fix this once things are sorted with ensembles
    warmstart_percentage_env_steps: 0.2
    warmstart_train_ratio: 1.5 # train_steps/env_step 

    # Training Itrs
    n_env_steps: null # Large number, to be set by specific env configs
    min_env_steps_per_round: 3500
    rounds_train_ratio: 1.5

    # Data collection
    data_collect_noise_std: 0.1

    # DP training
    eval_every_round: 10
    update_dp_every: 10
    n_dp_train_itrs: 1000
    n_traj_to_relabel_per_round: 64
    n_dp_traj_buffer_size: 64
    dp_expert_buffer_ratio: 0.5 # ablate

    # Discriminator
    use_discrim: True
    discrim_state_only: True
    upate_discrim_every: 100

robomimic:
  done_mode: 1 # can be 0, 1, 2
  action_repeat: 1
  shape_rewards: True
  
  dp:
    shared_mlp: []
    train_steps: 24000
    eval_freq: 12000

  env_max_steps:
    {lift: 1e5, can: 5e5, square: 5e5}
  env_time_limits:
    {lift: 100, can: 200, square: 200}

robocasa:
  done_mode: 1 # can be 0, 1, 2
  image_size: 256
  action_repeat: 3
  
  dp:
    shared_mlp: []
    train_steps: 24000
    eval_freq: 12000

  env_max_steps:
    { 
      stack: 3e5, door: 1e5, bread: 3e5,
      cereal: 5e5, nutassemblyround: 5e5, 
      breadcanfixed: 5e5, breadcanrandom: 5e5, 
    }

  env_time_limits:
    {stack: 150, door: 200, bread: 200,
    cereal: 150, nutassemblyround: 300,
    breadcanfixed: 400, breadcanrandom: 400}

maniskill:
  action_repeat: 2
  image_size: 64
  use_cpu_env: True
  
  dp:
    shared_mlp: []
    train_steps: 24000
    eval_freq: 12000
    
  env_max_steps:
    {pullcube: 1e5, liftpeg: 3e5, pokecube: 3e5}
  env_time_limits:
    {pullcube: 50, liftpeg: 150, pokecube: 100}

# Top Level Options
# Train DP
cfg_dp_training:
  dp:
    pretrained_ckpt: null

# Train DP with MPPI
cfg_dp_mppi:
  train_dp_mppi: True

debug:
  debug: True
  logdir: "debug_logs"
  pretrain: 1
  prefill: 1
  batch_size: 4
  batch_length: 8
  num_exp_trajs: 5
  num_exp_val_trajs: 1
  eval_episode_num: 0
  eval_num_runs: 3
  steps_per_batch: 10
  train_ratio: 50
  num_envs: 3
  num_buffer_transitions: 50
  visualize_eval: True

  dp:
    train_steps: 10
    eval_freq: 10
  
  encoder_net:
    train_steps: 10
    eval_interval: 2
  
  use_wandb: False

  mppi:
    # planning
    iterations: 2
    num_elites: 2
    temperature: 0.5
    num_samples: 32

  residual_training:
    # Warmup
    num_warmup_trajs: 3
    warmstart_critic_loss: -10000
    max_warmup_itrs: 100

    # Training
    num_trajs_per_round: 3
    n_training_rounds: 3
    n_steps_per_round: 100

  retrain_dp_params:
    new_trajs_to_collect: 3

  train_dp_mppi_params:
    # Warmstart
    force_warmup: True # TODO: change to false after formalizing critic
    n_env_steps: 230
    min_env_steps_per_round: 20
    update_dp_every: 2
    n_dp_train_itrs: 10
    eval_every_round: 2

    n_traj_to_relabel_per_round: 3
    n_dp_traj_buffer_size: 6

    use_discrim: true
    upate_discrim_every: 2